{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "from subprocess import check_output\n",
    "import os\n",
    "\n",
    "\n",
    "from time import time\n",
    "tik = time()\n",
    "tik_local = time()\n",
    "\n",
    "def mytime(tik_local):\n",
    "    tak = time()\n",
    "    print('all time is - ', tak-tik, '(sec)')\n",
    "    print('local time is - ', tak-tik_local, '(sec)')\n",
    "    return tak\n",
    "#procedure start example of def mytime\n",
    "#tik_local = mytime(tik_local)\n",
    "\n",
    "\n",
    "#Unique Data Analysis\n",
    "#процедура аналицирует количество одинаковых значений в столбце и если их больше чем unique_part*100%,\n",
    "#то создает список на удаление\n",
    "def unique_analysis(unique_part):\n",
    "    col_for_drop = []\n",
    "    for col in all_data.columns:\n",
    "        if (np.max(all_data[col].value_counts())/len(all_data[col]))>unique_part:\n",
    "            print(col,np.max(all_data[col].value_counts()), ' of ', len(all_data[col]), ' per cent is ', \n",
    "                  np.max(all_data[col].value_counts())*100//len(all_data[col]),'%')\n",
    "            col_for_drop.append(col)\n",
    "    print\n",
    "    return col_for_drop #возвращает список столбцов таблицы с количеством большим чем unique_part\n",
    "#procedure start example of def unique_analysis\n",
    "#col_for_drop = unique_analysis(0.8)\n",
    "\n",
    "\n",
    "#Missing Data analysis\n",
    "def missing_analysis(all_data):\n",
    "    all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "    missing_data = pd.DataFrame({'Missing Ratio (% of total all_data)' :all_data_na})\n",
    "    print(missing_data.head(20))\n",
    "#procedure start example of def unique_analysis\n",
    "#missing_analysis(all_data)\n",
    "\n",
    "\n",
    "#determinate train to numeric and categoric data\n",
    "# Take analysis of numeric and categorical columns\n",
    "def columns_len(all_data):\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    start_len_num = len(numeric_feats)\n",
    "    print('len of the numeric_feats is: ', start_len_num)\n",
    "    categorical_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\n",
    "    start_len_cat = len(categorical_feats)\n",
    "    print('len of the categorical_feats is: ', start_len_cat)\n",
    "    print('total len of all_data_feats is: ', len(all_data.columns))\n",
    "    return numeric_feats, categorical_feats\n",
    "\n",
    "\n",
    "#other def for determing the numeric and categiric features (columns of dataframe) as lists\n",
    "def list_columns(all_data):\n",
    "    cat_cols = [c for c in all_data.columns if all_data[c].dtype.name == 'object']\n",
    "    num_cols   = [c for c in all_data.columns if all_data[c].dtype.name != 'object']\n",
    "    return num_cols, cat_cols\n",
    "#procedure start example of def list_columns\n",
    "#num_cols, cat_cols = list_columns(all_data)\n",
    "\n",
    "\n",
    "# Определение вылетов по межквартильному размаху в столбцах с цифрами\n",
    "# Помечаем вылеты через дополнительные столбцы col +'Outlier'\n",
    "#создаем функцию, которая возвращает индекс выбросов\n",
    "def indicies_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3-q1\n",
    "    lower_bond = q1 - (iqr*1.5)\n",
    "    upper_bond = q3 + (iqr*1.5)\n",
    "    return np.where((x > upper_bond) | (x < lower_bond))\n",
    "#основная функция\n",
    "# делаем дополнительные бинарные столбы с признаком выброса по каждому числовому столбцу\n",
    "def outlier_mark_percentile(all_data):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    for col in num_cols:\n",
    "        pp = indicies_of_outliers(all_data[col])\n",
    "        list_index = pp[0]\n",
    "        if len(list_index)>1:\n",
    "            name_col = col +'Outlier'\n",
    "            all_data[name_col] = 0\n",
    "            all_data[name_col][list_index] = 1\n",
    "            print(len(list_index), ' --- ', all_data[name_col].sum(), ' --- ', name_col)\n",
    "#procedure start example of outlier_mark_percentile\n",
    "#outlier_mark_percentile(all_data)        \n",
    "\n",
    "\n",
    "\n",
    "# класстеризация поля '1stFlrSF', '2ndFlrSF' на 5 групп через метод к-средних\n",
    "# и запись распределение данных по группам в новую колонку\n",
    "from sklearn.cluster import KMeans\n",
    "def kmean_clust(dataframe, col1, col2, num_groups):  #dataframe, col1, col2 as string, num_groups as int\n",
    "    dataframe = dataframe[[col1, col2]]\n",
    "    #создаем кластеризатор по методу К средних\n",
    "    clusterer = KMeans(num_groups, random_state = 0)\n",
    "    #выполнить подгонку кластеризатора\n",
    "    clusterer.fit(dataframe)\n",
    "    #предсказать значения в новую колонку\n",
    "    dataframe[col1+col2+'kmean_clust'] = clusterer.predict(dataframe)\n",
    "\n",
    "\n",
    "#Снижение размерности с помощью выделения признаков (метод главных компонент)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def pca_features(all_data):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = StandardScaler().fit_transform(all_data[num_cols])\n",
    "    pca = PCA(n_components=0.99, whiten = True)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    print('исходное количество признаков:', features.shape[1])\n",
    "    print('сокращенное количество признаков (pca):', features_pca.shape[1])\n",
    "\n",
    "\n",
    "#Уменьшение количества признаков путем максимизации разделимости классов\n",
    "# до количества столбцов n_components, остальные считаются менее важными\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def cat_features(all_data, target, n_components):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[num_cols]\n",
    "    target = target\n",
    "    lda = LinearDiscriminantAnalysis(n_components =n_components)\n",
    "    features_lda = lda.fit(features, target).transform(features)\n",
    "    print('исходное количество признаков:', features.shape[1])\n",
    "    print('сокращенное количество признаков lda:', features_lda.shape[1])\n",
    "\n",
    "\n",
    "#уменьшения количества признаков на разряженных матрицах\n",
    "# надо доделать после one-hot coding на категорийных столбцах \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "def feat_unrise_onehot(all_data, target, n_components):    #n_components as int\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[num_cols]\n",
    "    target = target\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    feature_sparse = csr_matrix(features)\n",
    "    tsvd = TruncatedSVD(n_components = n_components)\n",
    "    features_sparse_tsvd = tsvd.fit(feature_sparse).transform(feature_sparse)\n",
    "    print('исходное количество признаков:', features.shape[1])\n",
    "    print('сокращенное количество признаков lda:', features_sparse_tsvd.shape[1])    \n",
    "\n",
    "\n",
    "#пороговая обработка дисперсии числовых признаков\n",
    "#в этом методе признаки не должны быть стандартизованы иначе у них у всех будет одинаковая дисперсия\n",
    "#суть метода в том, что высокодисперсные признаки несут больше информации чем низкодисперсные\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def feat_threshold(all_data, target):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[num_cols]\n",
    "    target = target\n",
    "    thresholder = VarianceThreshold(threshold = .5)\n",
    "    features_high_variance = thresholder.fit_transform(features)\n",
    "    print('исходное количество признаков:', features.shape[1])\n",
    "    print('сокращенное количество признаков lda:', features_high_variance.shape[1])\n",
    "    #print(features_high_variance[:5])\n",
    "    #взглянуть на дисперсию отобранных признаков\n",
    "    print(thresholder.fit(features).variances_)\n",
    "\n",
    "\n",
    "def corr_data(all_data, target):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[num_cols]\n",
    "    target = target    \n",
    "    #обработка высококоррелированных признаков\n",
    "    #создаем корреляционную матрицу\n",
    "    corr_matrix = features.corr().abs()\n",
    "    #выбираем верхний треугольник корреляционной матрицы (т.к. нижний симметричный)\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    #находим индекс столбцов признаком с корреляцией больше 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column]>0.95)]\n",
    "    print(to_drop)\n",
    "    # исключаем эти признаки\n",
    "    features.drop(to_drop, axis =1).head(3)\n",
    "    print('количество признаков:', features.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "#удаление нерелевантных признаков для классификации\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "def kbest_cat(all_data, target):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[cat_cols]\n",
    "    target = target\n",
    "    #for cаtegoric features\n",
    "    #конвертировать категориальные данные в целые числа\n",
    "    features = features.astype(int)\n",
    "    #отобрать два признака с наивысшими значениями \n",
    "    #статистического показателя хи-квадрат\n",
    "    chi2_selector = SelectKBest(chi2, k=2)\n",
    "    features_kbest = chi2_secetor.fit_transform(features, target)\n",
    "\n",
    "def kbest_num(all_data, target):\n",
    "    num_cols, cat_cols = list_columns(all_data)\n",
    "    features = all_data[num_cols]\n",
    "    target = target\n",
    "    #for nuneric features\n",
    "    #отобрать два признака с наивысшими значениями \n",
    "    #статистического показателя F\n",
    "    fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "    features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "def kbest_all(all_data, target):\n",
    "    #for all types features\n",
    "    from sklearn.feature_selection import SelectPercentile\n",
    "    #отобрать верхние 75% признаков с наивысшими значениями \n",
    "    #статистического показателя F\n",
    "    fvalue_selector = SelectPercentile(f_classif, percentile = 75)\n",
    "    features_kbest = fvalue_selector.fit_transform(all_data, target)\n",
    "\n",
    "def skewed_features(all_data):\n",
    "    #Skewed features\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = all_data.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    print(\"\\nSkew in numerical features: \\n\")\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    skewness.head(10)\n",
    "    #Box Cox Transformation of (highly) skewed features\n",
    "    skewness = skewness[abs(skewness) > 0.75]    # My_DS_work - was 0.75 my vers - 0.5\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "    from scipy.special import boxcox1p\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    #all_data[skewed_features] = np.log1p(all_data[skewed_features])\n",
    "    #Getting dummy categorical features\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "    print(all_data.shape)    \n",
    "    \n",
    "\n",
    "def rfecv_features(all_data, target, num_ranking):\n",
    "    #рекурсивное устранение признаков после нормирования признаков через бокс-кокс\n",
    "    # num_kanking - номер ранжирование признака от самого лучшего (1) до самого плохого, integer\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore', module = 'scipy', message='^internal gelsd')\n",
    "    ols = LinearRegression()\n",
    "    rfecv = RFECV(estimator=ols, step=1, scoring='neg_mean_squared_error')\n",
    "    rfecv.fit(features, target)\n",
    "    rfecv.transform(features)\n",
    "    print(rfecv.n_features_, 'количество наилучших признаков')\n",
    "    print(\"какие категории самые лучшие: \\n\", rfecv.support_)\n",
    "    print(\"ранжирование признака от самого лучшего (1) до самого плохого: \\n\", rfecv.ranking_)\n",
    "    ii_col = pd.concat([pd.DataFrame(all_data.columns, columns=['feature']), pd.DataFrame(rfecv.ranking_, columns=['ranking'])], axis=1)\n",
    "    ii_col = ii_col.sort_values(by=['ranking'])\n",
    "    best_features = ii_col[ii_col['ranking']<num_ranking]['feature']\n",
    "    #print(ii_col)\n",
    "    print(best_features)\n",
    "    return best_features   #as list\n",
    "\n",
    "\n",
    "# *********************  MODELS **********************************\n",
    "#neg_mean_squared_error for models\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "#\n",
    "#procedure start example of outlier_mark_percentile\n",
    "#score = rmsle_cv(lasso)\n",
    "#str_lasso = \"\\nLasso score: {:.4f} ({:.4f})\".format(score.mean(), score.std())\n",
    "#print(str_lasso)\n",
    "\n",
    "\n",
    "#*********************  Stacking models  *************************************\n",
    "#Simplest Stacking approach : Averaging base models\n",
    "#We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our \n",
    "#model and also to laverage encapsulation and code reuse (inheritance)\n",
    "#Averaged base models class\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   \n",
    "#Averaged base models score\n",
    "#We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models #in the mix.\n",
    "#****** procedure start example of AveregingModels:\n",
    "#averaged_models = AveragingModels(models = (ENet, GBoost, model_xgb, lasso))  # was - ENet, GBoost, KRR, lasso\n",
    "#score = rmsle_cv(averaged_models)\n",
    "#str_avr_model = \" Averaged base models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())\n",
    "#print(str_avr_model)\n",
    "\n",
    "\n",
    "#***************************  Stacking averaged Models Class   *****************************\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=15):      #was n_folds=5\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "#---------------------------\n",
    "#Stacking Averaged models Score\n",
    "#To make the two approaches comparable (by using the same number of models) , we just average Enet KRR \n",
    "#and Gboost, then we add lasso as meta-model.\n",
    "#****** procedure start example of StakingAveregingModels:\n",
    "#stacked_averaged_models = StackingAveragedModels(base_models = (ENet, ENet, GBoost, KRR), \n",
    "#                                                 meta_model = lasso)\n",
    "#score = rmsle_cv(stacked_averaged_models)\n",
    "#str_stacking = \"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())\n",
    "#print(str_stacking)\n",
    "\n",
    "\"\"\"\n",
    "#Final Training and Prediction\n",
    "#StackedRegressor\n",
    "\n",
    "stacked_averaged_models.fit(train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "print(rmsle(y_train, stacked_train_pred), ' str_ctackregr_rmsle')\n",
    "\n",
    "tik_local = mytime(tik_local)\n",
    "\n",
    "#XGBoost:\n",
    "\n",
    "model_xgb.fit(train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "print(rmsle(y_train, xgb_train_pred), ' str_xgb_rmsle ')\n",
    "\n",
    "tik_local = mytime(tik_local)\n",
    "\n",
    "#LightGBM:\n",
    "model_lgb.fit(train, y_train)\n",
    "lgb_train_pred = model_lgb.predict(train)\n",
    "lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
    "print(rmsle(y_train, lgb_train_pred), 'str_lgb_rmsle ')\n",
    "\n",
    "tik_local = mytime(tik_local)\n",
    "\n",
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print('stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15')\n",
    "print(rmsle(y_train,(stacked_train_pred*0.7 +xgb_train_pred*0.15 + lgb_train_pred*0.15 )))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#******************** EXAMPLES OF MODELS **************************\n",
    "#import libraries\n",
    "from sklearn.linear_model import BayesianRidge, LassoLarsIC, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LASSO Regression :\n",
    "#This model may be very sensitive to outliers. So we need to made it more robust on them. For that we \n",
    "#use the #sklearn's Robustscaler() method on pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "#Elastic Net Regression :  again made robust to outliers\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))\n",
    "\n",
    "#Kernel Ridge Regression :\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "#Gradient Boosting Regression :  With huber loss that makes it robust to outliers\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBoost = GradientBoostingRegressor(loss='huber', learning_rate=0.05, n_estimators=100, subsample=1.0, \n",
    "          criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "          min_weight_fraction_leaf=0.0, max_depth=4, min_impurity_decrease=0.0, \n",
    "          min_impurity_split=None, init=None, random_state=5, max_features='sqrt', \n",
    "          alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', \n",
    "          validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
    "\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_DTG = DecisionTreeRegressor(criterion='mae', splitter='best', max_depth=4, \n",
    "                                  min_samples_split=2, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                  presort=False)\n",
    "\n",
    "\n",
    "#SGDRegressor :\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "model_SGDR = SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001, \n",
    "                         l1_ratio=0.15, fit_intercept=True, max_iter=1000, \n",
    "                         tol=0.001, shuffle=True, verbose=0, epsilon=0.1, \n",
    "                         random_state=None, learning_rate='invscaling', \n",
    "                         eta0=0.01, power_t=0.25, early_stopping=False, \n",
    "                         validation_fraction=0.1, n_iter_no_change=5, \n",
    "                         warm_start=False, average=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
